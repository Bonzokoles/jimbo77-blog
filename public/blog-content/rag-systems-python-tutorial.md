# üîç RAG Systems w Pythonie - Kompletny Przewodnik 2025

## Wprowadzenie

**Retrieval-Augmented Generation (RAG)** to najwa≈ºniejsza architektura AI w 2025 roku. Pozwala LLM-om na dostƒôp do aktualnych, dok≈Çadnych informacji z Twoich w≈Çasnych baz danych. W tym artykule nauczysz siƒô budowaƒá production-ready RAG systemy od zera.

---

## Co To Jest RAG?

### Problem Ze Zwyk≈Çym LLM-em

```
User: "Jaka jest polityka zwrot√≥w?"
LLM: "Nie mam aktualnych informacji w mojej wiedzy treningowej"
‚ùå Wynik: Przestarza≈Çe lub b≈Çƒôdne informacje
```

### RozwiƒÖzanie: RAG

```
User Query
    ‚Üì
Retrieve: Szukaj w swojej bazie (wektory)
    ‚Üì
Augment: Dodaj kontekst do promptu
    ‚Üì
Generate: LLM odpowiada z kontekstem
    ‚Üì
‚úì Wynik: Dok≈Çadna, aktualna, uzasadniona odpowied≈∫
```

### Architektura RAG

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  INGESTION PIPELINE (Offline)           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Load Documents (PDF, Docs, Websites) ‚îÇ
‚îÇ 2. Chunk Text (Smart Chunking)          ‚îÇ
‚îÇ 3. Create Embeddings (Semantic)         ‚îÇ
‚îÇ 4. Store in Vector Database             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RETRIEVAL PIPELINE (Query Time)        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. User Query                           ‚îÇ
‚îÇ 2. Query Embedding                      ‚îÇ
‚îÇ 3. Similarity Search (Top K)            ‚îÇ
‚îÇ 4. Reranking (Optional but Important)   ‚îÇ
‚îÇ 5. Return Relevant Chunks               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GENERATION PIPELINE                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Build Prompt: Query + Context        ‚îÇ
‚îÇ 2. Call LLM                             ‚îÇ
‚îÇ 3. Stream Response                      ‚îÇ
‚îÇ 4. Return with Sources                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Krok 1: Ingestion - Przygotowanie Danych

### Instalacja

```bash
pip install langchain openai faiss-cpu sentence-transformers python-dotenv
```

### Load & Chunk Documents

```python
from langchain.document_loaders import PDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Za≈Çaduj PDF
loader = PDFLoader("documents/policy.pdf")
documents = loader.load()

# Smart Chunking (512 tokens, overlap 20%)
splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=102,
    separators=["\n\n", "\n", " ", ""]
)

chunks = splitter.split_documents(documents)
print(f"‚úì Created {len(chunks)} chunks")
```

### Create Embeddings

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.embeddings.huggingface import HuggingFaceEmbeddings

# Opcja 1: OpenAI (lepsze wyniki, p≈Çatne)
embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

# Opcja 2: Open Source (darmowe, lokalne)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Osad≈∫ chunki
chunk_embeddings = embeddings.embed_documents(
    [chunk.page_content for chunk in chunks]
)
print(f"‚úì Created embeddings for {len(chunk_embeddings)} chunks")
```

### Store in Vector Database

```python
from langchain.vectorstores import FAISS

# FAISS (szybki, lokalny)
vector_store = FAISS.from_documents(chunks, embeddings)
vector_store.save_local("faiss_index")

# Lub: Pinecone (cloud, skalabilne)
from langchain.vectorstores import Pinecone
import pinecone

pinecone.init(api_key="...", environment="...")
vector_store = Pinecone.from_documents(
    chunks, 
    embeddings, 
    index_name="rag-index"
)
```

---

## Krok 2: Retrieval - Szukanie Kontekstu

### Basic Retrieval

```python
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

# Load vector store
embeddings = OpenAIEmbeddings()
vector_store = FAISS.load_local("faiss_index", embeddings)

# Query
query = "Jaka jest polityka zwrot√≥w?"
results = vector_store.similarity_search(query, k=3)

for i, doc in enumerate(results):
    print(f"\n{i+1}. Score: {doc.metadata}")
    print(doc.page_content[:200])
```

### Hybrid Search (Keyword + Semantic)

```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.vectorstores import FAISS

# Vector Search (semantic)
vector_retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 5}
)

# Keyword Search (BM25)
keyword_retriever = BM25Retriever.from_documents(chunks)

# Combine both
ensemble_retriever = EnsembleRetriever(
    retrievers=[vector_retriever, keyword_retriever],
    weights=[0.6, 0.4]  # 60% semantic, 40% keyword
)

results = ensemble_retriever.get_relevant_documents(query)
```

### Reranking (Important!)

```python
from langchain.retrievers.contextual_compression import (
    ContextualCompressionRetriever
)
from langchain.retrievers.document_compressors import CohereReranker
from cohere import Client

# Initialize reranker
cohere_client = Client(api_key="...")
reranker = CohereReranker(client=cohere_client)

# Wrap retriever
compression_retriever = ContextualCompressionRetriever(
    base_compressor=reranker,
    base_retriever=ensemble_retriever
)

# Get best results
results = compression_retriever.get_relevant_documents(query)
```

---

## Krok 3: Generation - Tworzenie Odpowiedzi

### Simple Generation

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough

llm = ChatOpenAI(model="gpt-4-turbo", temperature=0)

# Build prompt template
template = """You are a helpful customer service assistant.
Use the provided context to answer the question.
If you don't know, say "I don't have this information".

Context:
{context}

Question: {question}

Answer:"""

prompt = ChatPromptTemplate.from_template(template)

# RAG Chain
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | llm
)

response = rag_chain.invoke("Jaka jest polityka zwrot√≥w?")
print(response.content)
```

### Generation with Citations

```python
from langchain.schema import Document

def generate_with_citations(query, docs, llm):
    # Create context with sources
    context = "\n\n".join([
        f"[Source {i}] {doc.page_content}"
        for i, doc in enumerate(docs)
    ])
    
    prompt = f"""Answer based on context:
    
{context}

Question: {query}

Provide answer with citations like [Source 1], [Source 2]"""
    
    response = llm.invoke(prompt)
    return response.content

answer = generate_with_citations(
    "Jaka jest polityka zwrot√≥w?",
    results,
    llm
)
```

---

## Komplety RAG System - 3 Frameworki

### 1Ô∏è‚É£ LangChain RAG

```python
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS

vector_store = FAISS.load_local("faiss_index", embeddings)

qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-4-turbo"),
    chain_type="stuff",  # or "map_reduce", "refine"
    retriever=vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )
)

response = qa_chain.run("Jaka jest polityka zwrot√≥w?")
print(response)
```

### 2Ô∏è‚É£ LlamaIndex RAG

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.llms.openai import OpenAI

# Load documents
documents = SimpleDirectoryReader("./documents").load_data()

# Create index
index = VectorStoreIndex.from_documents(documents)

# Query engine
query_engine = index.as_query_engine(
    similarity_top_k=3,
    response_mode="compact"
)

response = query_engine.query("Jaka jest polityka zwrot√≥w?")
print(response)
```

### 3Ô∏è‚É£ Custom RAG Pipeline (Full Control)

```python
from typing import List
from langchain.schema import Document
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS

class CustomRAG:
    def __init__(self, vector_store_path: str):
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(model="gpt-4-turbo")
        self.vector_store = FAISS.load_local(
            vector_store_path, 
            self.embeddings
        )
    
    def retrieve(self, query: str, k: int = 3) -> List[Document]:
        """Retrieve relevant documents"""
        return self.vector_store.similarity_search(query, k=k)
    
    def generate(self, query: str, docs: List[Document]) -> str:
        """Generate answer with context"""
        context = "\n".join([d.page_content for d in docs])
        
        prompt = f"""Answer based on context:

{context}

Question: {query}"""
        
        response = self.llm.invoke(prompt)
        return response.content
    
    def query(self, question: str) -> dict:
        """Full RAG pipeline"""
        docs = self.retrieve(question)
        answer = self.generate(question, docs)
        
        return {
            "answer": answer,
            "sources": [d.metadata for d in docs]
        }

# Usage
rag = CustomRAG("faiss_index")
result = rag.query("Jaka jest polityka zwrot√≥w?")
print(result["answer"])
print(f"Sources: {result['sources']}")
```

---

## Advanced Techniques

### Query Expansion

```python
def expand_query(query: str, llm) -> List[str]:
    """Generate alternative queries for better retrieval"""
    
    prompt = f"""Generate 3 alternative ways to ask this question:
    Original: {query}
    
    Alternatives:"""
    
    response = llm.invoke(prompt)
    queries = [query] + response.content.strip().split("\n")
    return queries

expanded = expand_query("Jaka jest polityka zwrot√≥w?", llm)
print(expanded)
# ['Jaka jest polityka zwrot√≥w?', 'Jak zwr√≥ciƒá produkt?', 'Jaki jest czas zwrotu?']
```

### Multi-Step Retrieval

```python
def multi_step_retrieval(query: str, retriever, llm, steps: int = 2):
    """Iterative retrieval for complex queries"""
    
    context = []
    current_query = query
    
    for step in range(steps):
        # Retrieve documents
        docs = retriever.get_relevant_documents(current_query)
        context.extend(docs)
        
        # Generate refined query for next step
        combined_context = "\n".join([d.page_content for d in docs])
        
        prompt = f"""Based on these documents, what follow-up question would help answer: {query}?

Context: {combined_context}

Follow-up question:"""
        
        follow_up = llm.invoke(prompt).content
        current_query = follow_up
    
    return context

# Usage
docs = multi_step_retrieval(
    "Ile kosztuje wysy≈Çka?",
    retriever,
    llm
)
```

### Reranking with Cross-Encoders

```python
from sentence_transformers import CrossEncoder

# Load cross-encoder for reranking
cross_encoder = CrossEncoder('cross-encoder/qnli-distilroberta-base')

def rerank_results(query: str, docs: List[Document], top_k: int = 3):
    """Rerank documents by relevance"""
    
    # Score all documents
    pairs = [[query, doc.page_content] for doc in docs]
    scores = cross_encoder.predict(pairs)
    
    # Sort by score
    scored_docs = list(zip(docs, scores))
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    
    # Return top K
    return [doc for doc, score in scored_docs[:top_k]]

# Usage
top_docs = rerank_results(query, all_docs)
```

---

## Real-World Example: Customer Support Bot

```python
from langchain.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.vectorstores import FAISS

# Load vector store
embeddings = OpenAIEmbeddings()
vector_store = FAISS.load_local("faiss_index", embeddings)

# Create chain
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(model="gpt-4-turbo", temperature=0),
    retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
    memory=memory,
    verbose=True
)

# Chat loop
while True:
    user_input = input("You: ")
    response = qa_chain({"question": user_input})
    print(f"Assistant: {response['answer']}\n")
```

---

## Production Considerations

### 1. **Chunking Strategy**
```
‚ùå Too small (100 tokens)  ‚Üí Lost context, more API calls
‚ùå Too large (2000 tokens) ‚Üí Slower retrieval, noise
‚úÖ Optimal (512 tokens, 20% overlap) ‚Üí Balance is key
```

### 2. **Embedding Model Choice**
| Model | Quality | Speed | Cost | Best For |
|-------|---------|-------|------|----------|
| OpenAI text-embedding-3-small | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | $$ | Production |
| all-MiniLM-L6-v2 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Free | Local |
| BGE-base | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Free | Good balance |

### 3. **Vector Database Selection**
```
FAISS ‚Üí Local, fast, good for <1M docs
Pinecone ‚Üí Managed, scalable, easy
Weaviate ‚Üí Open source, flexible
Milvus ‚Üí High performance, complex
```

### 4. **Monitoring & Evaluation**

```python
def evaluate_rag(test_queries: List[str], expected_answers: List[str]):
    """Evaluate RAG system"""
    
    from langchain.evaluation import load_evaluator
    
    evaluator = load_evaluator("qa")
    
    results = []
    for query, expected in zip(test_queries, expected_answers):
        # Generate answer
        retrieved_docs = retriever.get_relevant_documents(query)
        answer = generate(query, retrieved_docs)
        
        # Evaluate
        score = evaluator.evaluate_strings(
            prediction=answer,
            reference=expected,
            input=query
        )
        
        results.append(score)
    
    avg_score = sum(r["score"] for r in results) / len(results)
    print(f"‚úì Average Score: {avg_score}")
    return results
```

---

## Common Pitfalls & Solutions

| Problem | Przyczyna | RozwiƒÖzanie |
|---------|-----------|------------|
| **Low relevance** | Z≈Çy chunking | U≈ºyj semantic chunking |
| **Hallucinations** | Brak kontekstu | Dodaj retrieval validation |
| **Slow queries** | Du≈ºa baza wektor√≥w | U≈ºywaj reranking, filters |
| **Expensive** | Du≈ºo API calls | Cache embeddings, batch process |
| **Outdated info** | Stara baza wektor√≥w | Refresh regularly |

---

## Best Practices 2025

1. **Hybrid Search** - zawsze kombinuj semantic + keyword
2. **Reranking** - drugi etap filtrowania z cross-encoders
3. **Query Expansion** - generuj alternatywne pytania
4. **Caching** - cache embeddings i wyniki
5. **Monitoring** - track relevance, latency, costs
6. **Updating** - refresh knowledge base regularnie
7. **Citations** - zawsze pokazuj ≈∫r√≥d≈Ça
8. **Testing** - extensive evaluation framework

---

## Zasoby & Linki

- **LangChain RAG**: https://docs.langchain.com/rag
- **LlamaIndex**: https://docs.llamaindex.ai
- **Vector DB Comparison**: https://www.pinecone.io/learn/vector-database/
- **Embedding Models**: https://huggingface.co/spaces/mteb/leaderboard
- **RAG Evaluation**: https://github.com/explodinggradients/ragas

---

## Podsumowanie

RAG to **przysz≈Ço≈õƒá praktycznych aplikacji AI**. Umo≈ºliwia:

- ‚úÖ Dok≈Çadne, aktualne odpowiedzi
- ‚úÖ Przezroczyste ≈∫r√≥d≈Ça
- ‚úÖ Niska halucynacja
- ‚úÖ Kontrola nad wiedzƒÖ

**Zacznij dzisiaj - RAG system mo≈ºesz postawiƒá w godzinƒô!** ‚ö°

---

**Happy RAG Building!** üîç‚ú®